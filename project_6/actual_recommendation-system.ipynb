{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Общая информация по проекту.\n\nЦель проекта: изучение принципов работы алгоритмов рекомендательных систем, создание прототипа.\n\nЗадачи проекта:\n\n- Реализация EDA/FE\n\n- Использование след. моделей:  LightFM, SVD, BaselineOnly(ALS/SGD)\n\n- Создание прототипа\n\n- Получить выводы и дальнейшие рекомендации\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling # import profile for EDA\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are avaHBVCilable in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport surprise\nfrom surprise import Reader, Dataset\n\nfrom lightfm.cross_validation import random_train_test_split\n\nimport scipy.sparse as sparse\n\nfrom lightfm import LightFM\nfrom lightfm.cross_validation import random_train_test_split\nfrom lightfm.evaluation import auc_score, precision_at_k, recall_at_k\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nimport scipy.sparse as sparse\n\n\nfrom surprise import SVD, AlgoBase, KNNBasic\nfrom surprise import Reader, Dataset\nfrom surprise.model_selection import GridSearchCV\n\nfrom surprise import BaselineOnly\n\nfrom surprise import accuracy\nfrom surprise.model_selection import PredefinedKFold\n\nfrom lightfm import LightFM, cross_validation\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/recommendationsv4/train.csv')\ntest = pd.read_csv('/kaggle/input/recommendationsv4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/recommendationsv4/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/recommendationsv4/meta_Grocery_and_Gourmet_Food.json') as f:\n    meta_list = []\n    for line in f.readlines():\n        meta_list.append(json.loads(line))\n        \nmeta = pd.DataFrame(meta_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#склеим данные по признаку asin\ndataset = pd.merge(train, meta, on='asin')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, submission.shape, dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Тестовый и тернировочный дадасеты отличаются по размеру, удалим из train столбец summary\n    Удалим столбцы, которые вероятно не являются информативными: asin, vote, style, image_y, details \n    Удалми столбцы, имеющие много пропусков:  vote, style,image_x, also_view, price, also_buy, date, feature, fit,tech, similar_item \n    Удалим дублирующие столбцы: reviewerName, reviewTime \n    Удалим признак reviewText и Description  данные признаки могут быть информативными(но не хватает времени на  обработку)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#удалим лишние признаки\ntrain_json = dataset.drop(['vote', 'style','image_x', 'also_view', 'price', 'also_buy', 'date', 'feature', 'fit','tech1', 'similar_item','reviewerName', 'reviewTime','description','reviewText', 'vote', 'style', 'image_y','details','summary','asin'], axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#оцифуем столбец verifed\ndic_verified = {\n    True: 1,\n    False: 0\n}\ntrain_json['verified'] = train_json['verified'].map(dic_verified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# поработаем с признаком unixReviewTime, предположительно дата отзыва может влиять \nfrom datetime import datetime\ntsmin = train_json.unixReviewTime.min()\ntsmax = train_json.unixReviewTime.max()\ndate_25 = int(train_json.unixReviewTime.quantile(0.25))\ndate_50 = int(train_json.unixReviewTime.quantile(0.50))\ndate_75 = int(train_json.unixReviewTime.quantile(0.75))\ndatetime.utcfromtimestamp(tsmin).strftime('%Y-%m-%d %H:%M:%S'), datetime.utcfromtimestamp(tsmax).strftime('%Y-%m-%d %H:%M:%S'),datetime.utcfromtimestamp(date_75).strftime('%Y-%m-%d %H:%M:%S'), datetime.utcfromtimestamp(date_25).strftime('%Y-%m-%d %H:%M:%S'),datetime.utcfromtimestamp(date_50).strftime('%Y-%m-%d %H:%M:%S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#получили разбег по времени почти 18 лет, данный признак буд оцифровывать по следующему принципу квантилей(идею заимствовал)\ndef digitdate(date):\n    if date <= date_25: date = 1\n    elif date_25 < date <= date_50: date = 2\n    elif date_50 < date <= date_75: date = 3\n    elif date_75 < date: date = 4\n    return date      \n\ntrain_json['unixReviewTime'] = train_json['unixReviewTime'].apply(lambda x: digitdate(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#предобработка датасета завершена, теперь посмотрим графику, корелляции и тд \n#profile = pandas_profiling.ProfileReport(train)\n#profile.to_file(\"rec_sys.html\")\n#profile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### unixReviewTime \"условно хорошо\" коррелирует с itemid\n#### verified \"условно хорошо\" коррелирует с itemid\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#создадим дамми перменные, предварительно создав категорию other\nbrand_freqs = list(train_json.brand.value_counts())\ntop_brand_count = int(np.percentile(brand_freqs, 90))\nall_brand = train_json.brand.value_counts().index\ntop_brand = list(all_brand)[:top_brand_count]\nbrand_to_throw_away = list(set(all_brand) - set(top_brand))\ntrain_json.loc[train_json.brand.isin(brand_to_throw_away),'brand'] = 'other'\n\ntrain_json = pd.get_dummies(train_json, columns=['brand'], dummy_na=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FE"},{"metadata":{},"cell_type":"markdown","source":"### Подготовим доп признаки для метода fit¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# подготовим данные\nfeatures_user = train_json[['userid', 'verified']]\nfeatures_item = train_json[['itemid', 'unixReviewTime']]\ntrain_json_fe = train_json[['userid','itemid','rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# используем unixReviewTime, brand, для item_features\n# используем verified для user_features\n# сформируем списки для item_features формата 'feature_name: feature_value'\n\nitem_feat = []\nname = []\nvalue = []\nfor column in features_item.drop(['itemid'], axis=1):\n    name += [column]*len(features_item[column].unique())\n    value += list(features_item[column].unique())\nfor x,y in zip(name, value):\n    res = str(x)+ \":\" +str(y)\n    item_feat.append(res)\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сформируем списки для user_features формата 'feature_name: feature_value'\n\nuser_feat = []\nname = []\nvalue = []\nfor column in features_user.drop(['userid'], axis=1):\n    name += [column]*len(features_user[column].unique())\n    value += list(features_user[column].unique())\nfor x,y in zip(name, value):\n    res = str(x)+ \":\" +str(y)\n    user_feat.append(res)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightfm.data import Dataset\n# формируем датасет методом fit\n\ndataset_features = Dataset()\ndataset_features.fit(train_json_fe['userid'].unique(),train_json_fe['itemid'].unique(),user_features = user_feat,item_features = item_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# добавляем взаимодействия(interactions) используя метод build_item_features \n# создадим список необходимого для  build_item_features формата\n\nlist_for_build = []\nfor column in features_item.drop(['itemid'], axis=1):\n    list_for_build.append(column + ':')\n\ndef feature_value(my_list):\n    result = []\n    a = my_list\n    for x,y in zip(list_for_build,a):\n        res = str(x) +\"\"+ str(y)\n        result.append(res)\n    return result\n\nad_subset = features_item.drop(['itemid'], axis=1)\nad_list = [x.tolist() for x in ad_subset.values]\nitem_feature_list = []\n\nfor item in ad_list:\n    item_feature_list.append(feature_value(item))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_tuple = list(zip(features_item.itemid, item_feature_list))\n\nitem_features = dataset_features.build_item_features(item_tuple, normalize= False)\nitem_features.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#В итоге мы получили матрицу взаимодействий, где строки - это продукты, а столбцы - это фичи продуктов","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# аналогично сделаем для user_features\nlist_for_build = []\nfor column in features_user.drop(['userid'], axis=1):\n    list_for_build.append(column + ':')\n\nad_subset = features_user.drop(['userid'], axis=1)\nad_list = [x.tolist() for x in ad_subset.values]\nuser_feature_list = []\n\nfor user in ad_list:\n    user_feature_list.append(feature_value(user))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_tuple = list(zip(features_user.userid, user_feature_list))\nuser_features = dataset_features.build_user_features(user_tuple, normalize= False)\nuser_features.todense()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# для использования FE создадим словари с помощью которых по id в датасете LightFM мы сможем находить id в датасете train_json\nuser_id_map, user_feature_map, item_id_map, item_feature_map = dataset_features.mapping()\n(interactions, weights) = dataset_features.build_interactions([(x[0], x[1], x[2]) for x in train_json_fe.values])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LightFM with FE\nmodel_lfm_fe = LightFM(loss='warp')\nmodel_lfm_fe.fit(interactions, \n    user_features = user_features,\n    item_features = item_features, \n    sample_weight = weights, \n    epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ids = train_json_fe.userid.apply(lambda x: user_id_map[x])\nitem_ids = train_json_fe.itemid.apply(lambda x: item_id_map[x])\nprediction_lfm_fe = model_lfm_fe.predict(user_ids.values, item_ids.values, user_features=user_features, item_features=item_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.roc_auc_score(train_json_fe.rating,prediction_lfm_fe)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(train,random_state=32, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_coo = sparse.coo_matrix((train_data['rating'].astype(int),\n                                 (train_data['userid'],\n                                  train_data['itemid'])))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, test_data.shape, ratings_coo.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightFm"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_THREADS = 10 #число потоков\nNUM_COMPONENTS = 41 #число параметров вектора \nNUM_EPOCHS = 14 #число эпох обучения\n\nmodel = LightFM(learning_rate=0.12, loss='logistic',\n                no_components=NUM_COMPONENTS)\nmodel = model.fit(ratings_coo, epochs=NUM_EPOCHS, \n                  num_threads=NUM_THREADS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_lfm, test_lfm = cross_validation.random_train_test_split(ratings_coo, test_percentage=0.25)\n#model.fit(train, item_features=item_features, epochs=50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_LFM = model.predict(test_data.userid.values, test_data.itemid.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.roc_auc_score(test_data.rating,preds_LFM)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_LFM = model.predict(test.userid.values,\n                      test.itemid.values)\nnormalized_preds = (preds_LFM - preds_LFM.min())/(preds_LFM - preds_LFM.min()).max()\nsubmission['rating']= normalized_preds\nsubmission.to_csv('/kaggle/working/submission_log.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVD"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create matrix for svd/als\nfrom surprise import Reader, Dataset\nreader= Reader(rating_scale=(0,1))\ndata = Dataset.load_from_df(train_data[['userid','itemid','rating']], reader) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = SVD()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyperparametr tuning\nparam_grid = {'n_epochs': [5, 20], \n              'lr_all': [0.002, 0.01],\n              'reg_all': [0.1, 1]}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\ngs.fit(data)\nparams = gs.best_params['rmse']\n\nmodel_svd = SVD(n_epochs=params['n_epochs'],lr_all=params['lr_all'], reg_all=params['reg_all'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create model \ntrainingSet = data.build_full_trainset()\nmodel = model_svd.fit(trainingSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_svd = model.test(zip(test.userid.values, test.itemid.values, [0]*len(test.userid.values)))\nest = [i.est for i in pred_svd]\npred_sub = np.array(est)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['rating']= pred_sub\nsubmission.to_csv('/kaggle/working/submission_log.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BaselineOnly (ALS/SGD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# (https://surprise.readthedocs.io/en/stable/prediction_algorithms.html) - documentation\n\n# ALS Algoritm\nbsl_options = {'method': 'als',\n               'n_epochs': [5,10],\n               'reg_u': 15,\n               'reg_i': 10\n               }\nalgo = BaselineOnly(bsl_options=bsl_options)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learning model\ntrainingSet = data.build_full_trainset()\ntestSet = trainingSet.build_testset()\nmodel_als = algo.fit(trainingSet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred test for submission\npred_als = model_als.test(zip(test.userid.values, test.itemid.values, [0]*len(test.userid.values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#geting submission\nest = [i.est for i in pred_als]\npred_sub = np.array(est)\nsubmission['rating']= pred_sub\nsubmission.to_csv('/kaggle/working/submission_log.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SGD Algoritm\nbsl_options = {'method': 'sgd',\n               'n_epochs': 20,\n               'reg': 0.000262055315534682,\n               'learning_rate':0.0069826353714177785\n               }\n\nalgo = BaselineOnly(bsl_options=bsl_options)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learning model\ntrainingSet = data.build_full_trainset()\ntestSet = trainingSet.build_testset()\n\nalgo = BaselineOnly(bsl_options=bsl_options)\nmodel_sgd = algo.fit(trainingSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_sgd = model_sgd.test(zip(test.userid.values, test.itemid.values, [0]*len(test.userid.values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#geting submission\nest = [i.est for i in pred_sgd]\npred_sub = np.array(est)\nsubmission['rating']= pred_sub\nsubmission.to_csv('/kaggle/working/submission_log.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install auto-surprise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuning hyperparametrs with auto_surprise\nfrom auto_surprise.engine import Engine\nimport hyperopt\n\nengine = Engine(['svd', 'svdpp', 'knn_basic', 'knn_baseline'])\n# Start the trainer\nbest_algo, best_params, best_score, tasks = engine.train_json(\ndata=data,\ntarget_metric='test_rmse',\ncpu_time_limit=60*60*2,\nmax_evals=100,\nhpo_algo=hyperopt.tpe.suggest\n)\n# Build the model using the best algorithm and hyperparameters\nbest_model = engine.build_model(best_algo, best_params)\n\n#defined the required parameters\n#Best algorithm: baseline_only\n#Best hyperparameters: {'bsl_options': {'learning_rate': 0.0069826353714177785, 'method': 'sgd', 'reg': 0.000262055315534682}}\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KKN(CF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.itemid.nunique(), train_data.userid.nunique() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_sample = train_data.userid.sample(8000)\nitem_sample = train_data.itemid.sample(2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import KNNBasic\nfrom surprise import Reader, Dataset\n\nreader = Reader(rating_scale=(0, 1))\n#data = Dataset.load_from_df(train_data[['userid', 'itemid', 'rating']], reader)\n\ndata = Dataset.load_from_df(train_data[train_data.userid.isin(user_sample) &\n                              train_data.itemid.isin(item_sample)][['userid', 'itemid', 'rating']], reader)\n\nsim_options = {\n    'name': 'cosine',\n    'user_based': False\n}\n \nknn = KNNBasic()\ntrainingSet = data.build_full_trainset()\nmodel_knn = knn.fit(trainingSet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testSet = trainingSet.build_testset()\npredictions = model_knn.test(testSet)\n\npredict = model_knn.test(zip(test.userid.values, test.itemid.values, [0]*len(test.userid.values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"est = [i.est for i in predictions]\narr = np.array(est)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.roc_auc_score(test_data.rating,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"est = [i.est for i in preds]\narr = np.array(est)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Выводы"},{"metadata":{},"cell_type":"markdown","source":"1. EDA / FE\n    - Структурное описание датасета приведено в разделе Overview блока EDA\n    - Графическое и семантическое описание значений приведены в разделе Variables блока EDA\n    - Анализ корреляций приведен в разделе Correlation блока EDA\n\n1.1. Предобработка датасета включает:\nУдаление лишних признаков\nГрафический и корреляционный анализ\n\n1.2.Заключение по EDA: \nЗаметных корреляций не обнаружено за исключением unixReviewTime и ItemID, verified и itemid. Попробуем на основе этих данных создать новые признаки и использовать взаимодействие с userid, itemid\nТестирование модели на датасете с удаленными признаками (исключая itemid, userid) практически дало схожую оценку(ROC/AUC)\nСоздали dummy переменные на основе признака brand\n\n1.3.Создали два новых признака item_features на основе unixReviewTime и  user_features на основе verified. Однако полученный результат по метрике roc_auc сильно упал(до 0,52)\n\n2. ML\nML(без обработки на основе базового train) \nБыло использовано три модели: LightFM, SVD, ALS/SGD\nНаилучший результат показала модель LightFM - 0.75, \nДругие результаты: SVD - 0.734, SGD - 0.737, ALS - 0.729\nМодели обучались с подбором гиперпараметров.\n\nML(c обработкой на основе склеенного датасета) \n\n\n3. Протипирование должно показать реальные результаты работы моделей. \nне делалось\n\n4. Рекомендации. \nРасширить FE. Побровать применить нейроные сетиюПопробовать другие метрики. \nРазработать прототип и проверить работу модели на реальных данных.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}